{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karan\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global AVG_TAGS_PER_VIDEO, US_CA_GB_TOKEN_CORPUS, US_VIDEOS_DF, US_FINAL_DF\n",
    "global CA_VIDEOS_DF, CA_FINAL_DF, GB_VIDEOS_DF, GB_FINAL_DF, US_CA_GB_FINAL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the punctuations and set all characters to lowercase\n",
    "RE_PREPROCESS = r'\\W+|\\d+' #the regular expressions that matches all non-characters\n",
    "\n",
    "#get rid of punctuation and make everything lowercase\n",
    "#the code belows works by looping through the array of text\n",
    "#for a given piece of text we invoke the `re.sub` command where we pass in the regular expression, a space ' ' to\n",
    "#subsitute all the matching characters with\n",
    "#we then invoke the `lower()` method on the output of the re.sub command\n",
    "#to make all the remaining characters\n",
    "#the cleaned document is then stored in a list\n",
    "#once this list has been filed it is then stored in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_REMOVE_URLS = r'http\\S+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFeatures(desc):\n",
    "    try:\n",
    "        desc = re.sub(RE_REMOVE_URLS, ' ', desc)\n",
    "        return re.sub(RE_PREPROCESS, ' ', desc)\n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataFrame(data_frame, country_code='US'):\n",
    "    data_frame.sort_values(by=['video_id', 'trending_date'], ascending=True, inplace=True)\n",
    "    grouped_videos = data_frame.groupby(['video_id']).last().reset_index()\n",
    "    \n",
    "    #Reading categories from the json file depending on country_code\n",
    "    json_location = './data/' + country_code +'_category_id.json'\n",
    "    with open(json_location) as data_file:\n",
    "        data = json.load(data_file)    \n",
    "    categories = []\n",
    "    for item in data['items']:\n",
    "        category = {}\n",
    "        category['category_id'] = int(item['id'])\n",
    "        category['title'] = item['snippet']['title']\n",
    "        categories.append(category)\n",
    "\n",
    "    categories_df = pd.DataFrame(categories)\n",
    "    # Merging videos data with category data\n",
    "    final_df = grouped_videos.merge(categories_df, on = ['category_id'])\n",
    "    final_df.rename(columns={'title_y': 'category', 'title_x': 'video_name'}, inplace=True)\n",
    "    \n",
    "    # Creating a features column that consists all features used for prediction.\n",
    "    # Also creating a corpus column that consists of all data required to train the model.\n",
    "    final_df['video_features'] = ''\n",
    "    final_df['video_corpus'] = ''\n",
    "    \n",
    "    if final_df['video_name'].astype(str) is not None:\n",
    "        final_df['video_features'] += final_df['video_name'].astype(str)\n",
    "\n",
    "    if final_df['channel_title'].astype(str) is not None:\n",
    "        final_df['video_features'] += final_df['channel_title'].astype(str)\n",
    "        \n",
    "    if final_df['description'].astype(str) is not None:\n",
    "        final_df['video_features'] += final_df['description'].astype(str)\n",
    "    \n",
    "    final_df['video_corpus'] += final_df['video_features']\n",
    "    if final_df['tags'].astype(str) is not None:\n",
    "        final_df['video_corpus'] += final_df['tags'].astype(str)\n",
    "    \n",
    "        \n",
    "    final_df['video_features'] = final_df['video_features'].apply(processFeatures)\n",
    "    final_df['video_corpus'] = final_df['video_corpus'].apply(processFeatures)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonEngAndStopwords(documents):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    processed_corpus = []\n",
    "    for document in documents:\n",
    "        processed_document = []\n",
    "        for word in document.split():\n",
    "            try:\n",
    "                if word not in stopwords_list and word.encode(encoding='utf-8').decode('ascii'):\n",
    "                    processed_document.append(word)\n",
    "            except UnicodeDecodeError:\n",
    "                # Can log something here\n",
    "                pass\n",
    "        processed_corpus.append(processed_document)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(feature_corpus):\n",
    "    feature_corpus = [comment.lower() for comment in feature_corpus]\n",
    "    processed_feature_corpus = removeNonEngAndStopwords(feature_corpus)\n",
    "    return processed_feature_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(token_corpus, model_name = 'word2vec_model.w2v'):\n",
    "    model = gensim.models.Word2Vec(sentences=token_corpus, min_count=1, size = 32)\n",
    "    model.train(token_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.save(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendTags(word2vec_model, input_words = ['trump', 'president'], number_of_tags = 10, model_name = 'word2vec_model.w2v'):\n",
    "    global US_CA_GB_TOKEN_CORPUS\n",
    "    tags = []\n",
    "         \n",
    "    try:\n",
    "        word2vec_model = gensim.models.Word2Vec.load(model_name)\n",
    "        tags = word2vec_model.most_similar(positive=input_words, topn=number_of_tags)\n",
    "    except FileNotFoundError:\n",
    "        word2vec_model = trainModel(US_CA_GB_TOKEN_CORPUS, model_name)\n",
    "        try:\n",
    "            tags = word2vec_model.most_similar(positive=input_words, topn=number_of_tags)\n",
    "        except:\n",
    "            US_CA_GB_TOKEN_CORPUS.append(input_words)\n",
    "            word2vec_model.build_vocab(US_CA_GB_TOKEN_CORPUS, update=True)\n",
    "            word2vec_model.train(US_CA_GB_TOKEN_CORPUS, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.iter)\n",
    "            word2vec_model.save(model_name)\n",
    "            tags = word2vec_model.most_similar(positive=input_words, topn=number_of_tags)\n",
    "    except:\n",
    "        US_CA_GB_TOKEN_CORPUS.append(input_words)\n",
    "        word2vec_model.build_vocab(US_CA_GB_TOKEN_CORPUS, update=True)\n",
    "        word2vec_model.train(US_CA_GB_TOKEN_CORPUS, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.iter)\n",
    "        word2vec_model.save(model_name)\n",
    "        tags = word2vec_model.most_similar(positive=input_words, topn=number_of_tags)\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAvgTagsPerVideo():\n",
    "    total_tags = 0\n",
    "    for tag_list in US_CA_GB_FINAL_DF['tags'].values:\n",
    "        total_tags += len(tag_list.split('|'))\n",
    "    return math.ceil(total_tags/len(US_CA_GB_FINAL_DF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the algorithm for US, CA, and GB videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeAndFetchRecommendations(video_name = None, channel_title = None, video_category = None, description = None):\n",
    "    global US_VIDEOS_DF, US_FINAL_DF, CA_VIDEOS_DF, CA_FINAL_DF, GB_VIDEOS_DF, GB_FINAL_DF\n",
    "    global US_CA_GB_FINAL_DF, US_CA_GB_FINAL_DF, AVG_TAGS_PER_VIDEO, US_CA_GB_TOKEN_CORPUS\n",
    "    US_VIDEOS_DF = pd.read_csv('./data/USvideos.csv')\n",
    "    US_FINAL_DF = processDataFrame(US_VIDEOS_DF, country_code='US')\n",
    "    \n",
    "    CA_VIDEOS_DF = pd.read_csv('./data/CAvideos.csv')\n",
    "    CA_FINAL_DF = processDataFrame(CA_VIDEOS_DF, country_code='CA')\n",
    "    \n",
    "    GB_VIDEOS_DF = pd.read_csv('./data/GBvideos.csv')\n",
    "    GB_FINAL_DF = processDataFrame(GB_VIDEOS_DF, country_code='GB')\n",
    "        \n",
    "    US_CA_GB_FINAL_DF = pd.concat([US_FINAL_DF, CA_FINAL_DF, GB_FINAL_DF])\n",
    "    US_CA_GB_FINAL_DF.reset_index(inplace=True)\n",
    "    \n",
    "    US_CA_GB_TOKEN_CORPUS = processCorpus(US_CA_GB_FINAL_DF['video_corpus'].values)\n",
    "    US_CA_GB_FINAL_DF['video_features'] = processCorpus(US_CA_GB_FINAL_DF['video_features'].values)\n",
    "    US_CA_GB_FINAL_DF['video_corpus'] = US_CA_GB_TOKEN_CORPUS\n",
    "        \n",
    "    AVG_TAGS_PER_VIDEO = calculateAvgTagsPerVideo()\n",
    "    AVG_TAGS_PER_VIDEO = 50\n",
    "    word2vec_model = None\n",
    "    \n",
    "    input_list = []\n",
    "    if (video_name is not None or channel_title is not None or \n",
    "        video_category is not None or description is not None):\n",
    "        frontEndInput = video_name + channel_title + video_category + description\n",
    "        for word in frontEndInput.split(' '):\n",
    "            if word.lower() not in stopwords.words('english'):\n",
    "                input_list.append(word.lower())\n",
    "    \n",
    "    if input_list != []:\n",
    "        return recommendTags(word2vec_model, input_words=input_list, \n",
    "                         number_of_tags=AVG_TAGS_PER_VIDEO, \n",
    "                         model_name = 'word2vec_model.w2v')\n",
    "    \n",
    "    return recommendTags(word2vec_model, input_words=['amazon', 'microsoft'], \n",
    "                         number_of_tags=AVG_TAGS_PER_VIDEO, \n",
    "                         model_name = 'word2vec_model.w2v')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = initializeAndFetchRecommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ibooks', 0.9179543256759644),\n",
       " ('kindle', 0.8989537954330444),\n",
       " ('floatplane', 0.8269237279891968),\n",
       " ('etc', 0.8128304481506348),\n",
       " ('ssd', 0.7979328632354736),\n",
       " ('target', 0.7923094034194946),\n",
       " ('cams', 0.7914948463439941),\n",
       " ('paperback', 0.7900267839431763),\n",
       " ('nvme', 0.7857794761657715),\n",
       " ('books', 0.7680275440216064),\n",
       " ('gtx', 0.7638066411018372),\n",
       " ('personalized', 0.7622551918029785),\n",
       " ('forum', 0.7592703104019165),\n",
       " ('newegg', 0.7498676776885986),\n",
       " ('affiliates', 0.7471767067909241),\n",
       " ('canceling', 0.7463813424110413),\n",
       " ('livre', 0.7437158823013306),\n",
       " ('gb', 0.7416175603866577),\n",
       " ('amd', 0.7372931241989136),\n",
       " ('tablet', 0.734533965587616),\n",
       " ('windows', 0.7330909967422485),\n",
       " ('harddrives', 0.729610800743103),\n",
       " ('norder', 0.7295733690261841),\n",
       " ('webstore', 0.7268610000610352),\n",
       " ('gamestop', 0.7238417267799377),\n",
       " ('namazon', 0.7206636667251587),\n",
       " ('macbook', 0.7167049050331116),\n",
       " ('linking', 0.7165277004241943),\n",
       " ('amazn', 0.7164859175682068),\n",
       " ('ataiii', 0.7134222388267517),\n",
       " ('nskybound', 0.7092013359069824),\n",
       " ('mavic', 0.7091645002365112),\n",
       " ('naugusta', 0.7060986757278442),\n",
       " ('walmart', 0.704188883304596),\n",
       " ('sd', 0.6974234580993652),\n",
       " ('pcie', 0.6939233541488647),\n",
       " ('amzon', 0.692742109298706),\n",
       " ('nook', 0.6910362243652344),\n",
       " ('cs', 0.6901466846466064),\n",
       " ('bluetooth', 0.6882288455963135),\n",
       " ('evansthe', 0.6858484148979187),\n",
       " ('pro', 0.6831679940223694),\n",
       " ('thunderbolt', 0.6821900606155396),\n",
       " ('artwork', 0.6816677451133728),\n",
       " ('tb', 0.6805330514907837),\n",
       " ('mercis', 0.6790754795074463),\n",
       " ('inmates', 0.6766890287399292),\n",
       " ('referral', 0.6754201054573059),\n",
       " ('firecuda', 0.6739603877067566),\n",
       " ('vergegoogle', 0.6729423403739929),\n",
       " ('nevga', 0.6715783476829529),\n",
       " ('samsung', 0.6710525751113892),\n",
       " ('reviewthe', 0.6676338911056519),\n",
       " ('ngoogle', 0.6663147211074829),\n",
       " ('hint', 0.6648721694946289),\n",
       " ('ugh', 0.663577139377594),\n",
       " ('curved', 0.6629935503005981),\n",
       " ('unbox', 0.6629334688186646),\n",
       " ('radeon', 0.6619371175765991),\n",
       " ('nasus', 0.6613042950630188),\n",
       " ('pixel', 0.6608790755271912),\n",
       " ('ks', 0.6602219343185425),\n",
       " ('nvidia', 0.660101592540741),\n",
       " ('upgrade', 0.6587289571762085),\n",
       " ('comdates', 0.6554298400878906),\n",
       " ('nintel', 0.6549253463745117),\n",
       " ('nmsi', 0.6547927856445312),\n",
       " ('embark', 0.653622031211853),\n",
       " ('brownleepixelbook', 0.6524723768234253),\n",
       " ('carbohydrates', 0.6519683599472046),\n",
       " ('soin', 0.6519426107406616),\n",
       " ('ndji', 0.6503479480743408),\n",
       " ('device', 0.6486192941665649),\n",
       " ('tech', 0.6481252908706665),\n",
       " ('participant', 0.6479985117912292),\n",
       " ('boil', 0.6479411125183105),\n",
       " ('nprecios', 0.6473987102508545),\n",
       " ('surface', 0.6468157172203064),\n",
       " ('involves', 0.6455858945846558),\n",
       " ('hdcrazy', 0.6446309089660645),\n",
       " ('ddr', 0.6443470120429993),\n",
       " ('nstream', 0.6439002156257629),\n",
       " ('ideapad', 0.640117883682251),\n",
       " ('galaxy', 0.6374974250793457),\n",
       " ('porsche', 0.635732114315033),\n",
       " ('einride', 0.6355226635932922),\n",
       " ('monitor', 0.635096549987793),\n",
       " ('dojowhich', 0.633974552154541),\n",
       " ('bbkivines', 0.6330907344818115),\n",
       " ('android', 0.632758617401123),\n",
       " ('lenovo', 0.6325658559799194),\n",
       " ('monitors', 0.632486879825592),\n",
       " ('hanson', 0.6323772668838501),\n",
       " ('tipsthanks', 0.6315432190895081),\n",
       " ('deluxe', 0.6295648813247681),\n",
       " ('ps', 0.6290892362594604),\n",
       " ('lionelmedia', 0.6285711526870728),\n",
       " ('apple', 0.6276111602783203),\n",
       " ('lanarose', 0.6270608901977539),\n",
       " ('multi', 0.6268960237503052)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_file = open(\"recommendations.txt\",\"w+\")\n",
    "for recommendation in recommendations:\n",
    "    the_file.write(recommendation[0] + ' ')\n",
    "the_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emphasis', 0.8339278697967529),\n",
       " ('medications', 0.8214027881622314),\n",
       " ('grandparent', 0.7855393290519714),\n",
       " ('fetus', 0.7847536206245422),\n",
       " ('unstructured,', 0.7833181619644165),\n",
       " ('constant', 0.7744555473327637),\n",
       " ('unused', 0.7670623064041138),\n",
       " ('metropolitan', 0.7617587447166443),\n",
       " ('redbull', 0.7607500553131104),\n",
       " ('cosmopolitan', 0.7585783004760742),\n",
       " ('netanyahu', 0.7550094127655029),\n",
       " ('mining.', 0.7532414197921753),\n",
       " ('abundant', 0.7525558471679688),\n",
       " ('reclassified', 0.7496532201766968),\n",
       " ('yanes', 0.747504711151123),\n",
       " ('structured', 0.7473931312561035),\n",
       " ('certified', 0.7473263740539551),\n",
       " ('palace_fullmix', 0.7466326951980591),\n",
       " ('sachet', 0.7462137341499329)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initializeAndFetchRecommendations(video_name = 'What is data science',\n",
    "                                  channel_title = 'CNN', \n",
    "                                  video_category = 'Education', \n",
    "                                  description = 'data science related')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the dataset into training (80%) and testing sets (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=13579)\n",
    "us_ca_gb_final_df_shuffled = US_CA_GB_FINAL_DF.iloc[np.random.permutation(len(US_CA_GB_FINAL_DF))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.80\n",
    "us_ca_gb_df_train = us_ca_gb_final_df_shuffled[:int((train_size)*len(us_ca_gb_final_df_shuffled))]\n",
    "us_ca_gb_df_test = us_ca_gb_final_df_shuffled[int((train_size)*len(us_ca_gb_final_df_shuffled)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    w2v_train_model = gensim.models.Word2Vec.load('w2v_train_model.w2v')\n",
    "except FileNotFoundError:\n",
    "    w2v_train_model = gensim.models.Word2Vec(sentences=us_ca_gb_df_train['video_corpus'], min_count=1, size = 32)\n",
    "    w2v_train_model.train(us_ca_gb_df_train['video_corpus'].values, total_examples=w2v_train_model.corpus_count, epochs=w2v_train_model.iter)\n",
    "    w2v_train_model.save('w2v_train_model.w2v')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_ca_gb_df_test = us_ca_gb_df_test[us_ca_gb_df_test['video_features'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_tags = []\n",
    "for idx in us_ca_gb_df_test.index:\n",
    "    video_features = us_ca_gb_df_test.loc[idx, 'video_features']\n",
    "    tag_probability_list = recommendTags(w2v_train_model, input_words=video_features, \n",
    "                                         number_of_tags=AVG_TAGS_PER_VIDEO, \n",
    "                                         model_name = 'w2v_train_model.w2v')\n",
    "    predicted_tags.append([tag[0] for tag in tag_probability_list if len(tag_probability_list) != 0])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_ca_gb_df_test['predicted_tags'] = predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_ca_gb_df_test['tags'] = us_ca_gb_df_test['tags'].apply(processFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found:  157\n",
      "Accuracy:  0.2599337748344371\n"
     ]
    }
   ],
   "source": [
    "match_found = 0\n",
    "count = 0\n",
    "for idx in us_ca_gb_df_test.index:\n",
    "    tag_list = us_ca_gb_df_test.loc[idx,'tags'].lower()\n",
    "    tag_list = tag_list.split(' ')\n",
    "    predicted_tag_list = us_ca_gb_df_test.loc[idx, 'predicted_tags']\n",
    "    \n",
    "    for i in range(len(tag_list)):\n",
    "        if tag_list[i] in predicted_tag_list:\n",
    "            match_found += 1\n",
    "            break\n",
    "    count += 1\n",
    "print('Match found: ', match_found )\n",
    "print('Accuracy: ', match_found/len(us_ca_gb_df_test))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeSimilarity(word1, word2):\n",
    "    try:\n",
    "        return w2v_train_model.wv.similarity(word1, word2)\n",
    "    except:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating similarity between predicted and actual tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Value:  0.305682692008\n"
     ]
    }
   ],
   "source": [
    "for idx in us_ca_gb_df_test.index:\n",
    "    tag_list = us_ca_gb_df_test.loc[idx,'tags'].lower()\n",
    "    tag_list = tag_list.split(' ')\n",
    "    predicted_tag_list = us_ca_gb_df_test.loc[idx, 'predicted_tags']\n",
    "    avg_similarity_per_row = 0\n",
    "    avg_similarity_scores = []\n",
    "    \n",
    "    for predicted_tag in predicted_tag_list:\n",
    "        similarity_score = -2\n",
    "        for tag in tag_list:\n",
    "            similarity_score = max(similarity_score, computeSimilarity(predicted_tag, tag))\n",
    "        avg_similarity_per_row += similarity_score\n",
    "    \n",
    "    avg_similarity_scores.append(avg_similarity_per_row / len(tag_list))\n",
    "\n",
    "cosine_similarity_value =sum(avg_similarity_scores)/len(avg_similarity_scores)\n",
    "print('Similarity Value: ', cosine_similarity_value)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_file = open(\"corpus_of_strings.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ''\n",
    "for token_list in US_CA_GB_TOKEN_CORPUS:\n",
    "    for token in token_list:\n",
    "        input += token + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2076487"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_file.write(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
