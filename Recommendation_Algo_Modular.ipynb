{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitTags(tag_list):\n",
    "    tag_list = tag_list.split('|')\n",
    "    output = ''\n",
    "    for tag in tag_list:\n",
    "        output += tag\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of the punctuations and set all characters to lowercase\n",
    "RE_PREPROCESS = r'\\W+|\\d+' #the regular expressions that matches all non-characters\n",
    "\n",
    "#get rid of punctuation and make everything lowercase\n",
    "#the code belows works by looping through the array of text\n",
    "#for a given piece of text we invoke the `re.sub` command where we pass in the regular expression, a space ' ' to\n",
    "#subsitute all the matching characters with\n",
    "#we then invoke the `lower()` method on the output of the re.sub command\n",
    "#to make all the remaining characters\n",
    "#the cleaned document is then stored in a list\n",
    "#once this list has been filed it is then stored in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processFeatures(desc):\n",
    "    try:\n",
    "        return re.sub(RE_PREPROCESS, ' ', desc)\n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processDataFrame(data_frame, country_code='US'):\n",
    "    data_frame.sort_values(by=['video_id', 'trending_date'], ascending=True, inplace=True)\n",
    "    grouped_videos = data_frame.groupby(['video_id']).last().reset_index()\n",
    "    \n",
    "    #Reading categories from the json file depending on country_code\n",
    "    json_location = './data/' + country_code +'_category_id.json'\n",
    "    with open(json_location) as data_file:\n",
    "        data = json.load(data_file)    \n",
    "    categories = []\n",
    "    for item in data['items']:\n",
    "        category = {}\n",
    "        category['category_id'] = int(item['id'])\n",
    "        category['title'] = item['snippet']['title']\n",
    "        categories.append(category)\n",
    "\n",
    "    categories_df = pd.DataFrame(categories)\n",
    "    # Merging videos data with category data\n",
    "    final_df = grouped_videos.merge(categories_df, on = ['category_id'])\n",
    "    final_df.rename(columns={'title_y': 'category', 'title_x': 'video_name'}, inplace=True)\n",
    "    \n",
    "    # Splitting the tags by pipe (|) character\n",
    "    final_df['tags'] = final_df['tags'].apply(splitTags)\n",
    "    \n",
    "    # Creating a features column that consists all features used for prediction.\n",
    "    final_df['video_features'] = final_df['tags'].astype(str) + final_df['video_name'].astype(str) \\\n",
    "                        + final_df['channel_title'].astype(str) + final_df['description'] + final_df['category']\n",
    "        \n",
    "    final_df['video_features'] = final_df['video_features'].apply(processFeatures)\n",
    "    final_df['video_features'] = final_df['video_features'].apply(processFeatures)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processCorpus(feature_corpus):\n",
    "    feature_corpus = [comment.lower() for comment in feature_corpus]\n",
    "    parser = spacy.load('en')\n",
    "    processed_feature_corpus = [parser(feature) for feature in feature_corpus]\n",
    "    token_corpus = [nltk.word_tokenize(str(feature)) for feature in processed_feature_corpus]\n",
    "    return token_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(token_corpus):\n",
    "    model = gensim.models.Word2Vec(sentences=token_corpus, min_count=1, size = 32)\n",
    "    model.train(token_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.save('word2vec_model.w2v')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommendTags(token_corpus, input_words = ['trump', 'president']):\n",
    "    word2vec_model = gensim.models.Word2Vec.load('word2vec_model.w2v')\n",
    "    tags = []\n",
    "    try:\n",
    "        tags = word2vec_model.most_similar(positive=input_words)\n",
    "    except:\n",
    "        token_corpus.append(input_words)\n",
    "        word2vec_model.build_vocab(token_corpus, update=True)\n",
    "        word2vec_model.train(token_corpus, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.iter)\n",
    "        word2vec_model.save('word2vec_model.w2v')\n",
    "        tags = word2vec_model.most_similar(positive=input_words)\n",
    "    \n",
    "    return token_corpus, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the algorithm for US videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_videos_df = pd.read_csv('./data/USvideos.csv')\n",
    "us_final_df = processDataFrame(us_videos_df, country_code='US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_videos_df = pd.read_csv('./data/CAvideos.csv')\n",
    "ca_final_df = processDataFrame(ca_videos_df, country_code='CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_videos_df = pd.read_csv('./data/GBvideos.csv')\n",
    "gb_final_df = processDataFrame(gb_videos_df, country_code='GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_ca_gb_final_df = pd.concat([us_final_df, ca_final_df, gb_final_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Warning: no model found for 'en'\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_ca_gb_token_corpus = processCorpus(us_ca_gb_final_df['video_features'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3029"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(us_ca_gb_token_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x2004aa02d68>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainModel(us_ca_gb_token_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_ca_gb_token_corpus, tags = recommendTags(us_ca_gb_token_corpus, input_words=['trump', 'president','karan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('putin', 0.8798644542694092),\n",
       " ('leann', 0.8748385906219482),\n",
       " ('donald', 0.8641480803489685),\n",
       " ('administration', 0.8589312434196472),\n",
       " ('cumbre', 0.8583245873451233),\n",
       " ('verifies', 0.856847882270813),\n",
       " ('meddling', 0.8502160906791687),\n",
       " ('roa', 0.84947669506073),\n",
       " ('lleg√≥', 0.8474310636520386),\n",
       " ('firestorm', 0.8454605340957642)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
