{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitTags(tag_list):\n",
    "    tag_list = tag_list.split('|')\n",
    "    output = ''\n",
    "    for tag in tag_list:\n",
    "        output += tag\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of the punctuations and set all characters to lowercase\n",
    "RE_PREPROCESS = r'\\W+|\\d+' #the regular expressions that matches all non-characters\n",
    "\n",
    "#get rid of punctuation and make everything lowercase\n",
    "#the code belows works by looping through the array of text\n",
    "#for a given piece of text we invoke the `re.sub` command where we pass in the regular expression, a space ' ' to\n",
    "#subsitute all the matching characters with\n",
    "#we then invoke the `lower()` method on the output of the re.sub command\n",
    "#to make all the remaining characters\n",
    "#the cleaned document is then stored in a list\n",
    "#once this list has been filed it is then stored in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processFeatures(desc):\n",
    "    try:\n",
    "        return re.sub(RE_PREPROCESS, ' ', desc)\n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processDataFrame(data_frame, country_code='US'):\n",
    "    data_frame.sort_values(by=['video_id', 'trending_date'], ascending=True, inplace=True)\n",
    "    grouped_videos = data_frame.groupby(['video_id']).last().reset_index()\n",
    "    \n",
    "    #Reading categories from the json file depending on country_code\n",
    "    json_location = './data/' + country_code +'_category_id.json'\n",
    "    with open(json_location) as data_file:\n",
    "        data = json.load(data_file)    \n",
    "    categories = []\n",
    "    for item in data['items']:\n",
    "        category = {}\n",
    "        category['category_id'] = int(item['id'])\n",
    "        category['title'] = item['snippet']['title']\n",
    "        categories.append(category)\n",
    "\n",
    "    categories_df = pd.DataFrame(categories)\n",
    "    # Merging videos data with category data\n",
    "    final_df = grouped_videos.merge(categories_df, on = ['category_id'])\n",
    "    final_df.rename(columns={'title_y': 'category', 'title_x': 'video_name'}, inplace=True)\n",
    "    \n",
    "    # Splitting the tags by pipe (|) character\n",
    "    final_df['tags'] = final_df['tags'].apply(splitTags)\n",
    "    \n",
    "    # Creating a features column that consists all features used for prediction.\n",
    "    final_df['video_features'] = final_df['tags'].astype(str) + final_df['video_name'].astype(str) \\\n",
    "                        + final_df['channel_title'].astype(str) + final_df['description'] + final_df['category']\n",
    "        \n",
    "    final_df['video_features'] = final_df['video_features'].apply(processFeatures)\n",
    "    final_df['video_features'] = final_df['video_features'].apply(processFeatures)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopwords(documents):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    print(stopwords_list)\n",
    "    processed_corpus = []\n",
    "    for document in documents:\n",
    "        processed_document = []\n",
    "        for word in document.split():\n",
    "            if word not in stopwords_list:\n",
    "                processed_document.append(word)\n",
    "        processed_corpus.append(processed_document)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processCorpus(feature_corpus):\n",
    "    feature_corpus = [comment.lower() for comment in feature_corpus]\n",
    "    processed_feature_corpus = removeStopwords(feature_corpus)\n",
    "    # processed_feature_corpus = [nltk.word_tokenize(str(feature)) for feature in feature_corpus]\n",
    "    return processed_feature_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(token_corpus):\n",
    "    model = gensim.models.Word2Vec(sentences=token_corpus, min_count=1, size = 32)\n",
    "    model.train(token_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.save('word2vec_model.w2v')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommendTags(token_corpus, input_words = ['trump', 'president']):\n",
    "    word2vec_model = gensim.models.Word2Vec.load('word2vec_model.w2v')\n",
    "    tags = []\n",
    "    try:\n",
    "        tags = word2vec_model.most_similar(positive=input_words)\n",
    "    except:\n",
    "        token_corpus.append(input_words)\n",
    "        word2vec_model.build_vocab(token_corpus, update=True)\n",
    "        word2vec_model.train(token_corpus, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.iter)\n",
    "        word2vec_model.save('word2vec_model.w2v')\n",
    "        tags = word2vec_model.most_similar(positive=input_words)\n",
    "    \n",
    "    return token_corpus, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the algorithm for US videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_videos_df = pd.read_csv('./data/USvideos.csv')\n",
    "us_final_df = processDataFrame(us_videos_df, country_code='US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_videos_df = pd.read_csv('./data/CAvideos.csv')\n",
    "ca_final_df = processDataFrame(ca_videos_df, country_code='CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_videos_df = pd.read_csv('./data/GBvideos.csv')\n",
    "gb_final_df = processDataFrame(gb_videos_df, country_code='GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_ca_gb_final_df = pd.concat([us_final_df, ca_final_df, gb_final_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_ca_gb_token_corpus = processCorpus(us_ca_gb_final_df['video_features'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3029"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(us_ca_gb_token_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x20065183e80>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainModel(us_ca_gb_token_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_ca_gb_token_corpus, tags = recommendTags(us_ca_gb_token_corpus, input_words=['saturday', 'night'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('snl', 0.9108177423477173),\n",
       " ('aneurysm', 0.8981908559799194),\n",
       " ('unhhhh', 0.8891002535820007),\n",
       " ('crespo', 0.8836565017700195),\n",
       " ('showings', 0.8795809745788574),\n",
       " ('meyersseth', 0.8729174137115479),\n",
       " ('tent', 0.8697624206542969),\n",
       " ('wednesday', 0.8695673942565918),\n",
       " ('seth', 0.8645941019058228),\n",
       " ('macysparade', 0.8599853515625)]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
